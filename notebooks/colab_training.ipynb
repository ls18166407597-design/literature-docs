{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {"id": "header"},
      "source": [
        "# 文献检索系统模型训练\n",
        "\n",
        "本笔记本自动从GitHub加载文档，无需手动上传。\n",
        "\n",
        "## 功能特点\n",
        "- 自动从GitHub克隆文档\n",
        "- 使用免费GPU加速训练\n",
        "- 支持PDF、DOCX、TXT、MD格式\n",
        "- 训练完成后自动保存"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {"id": "clone_repo"},
      "source": [
        "## 1. 克隆GitHub仓库"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {"id": "clone_repo_code"},
      "outputs": [],
      "source": [
        "# 克隆您的GitHub仓库\n",
        "!git clone https://github.com/ls18166407597-design/literature-docs.git\n",
        "\n",
        "# 查看文档\n",
        "import os\n",
        "if os.path.exists('/content/literature-docs/documents'):\n",
        "    files = os.listdir('/content/literature-docs/documents')\n",
        "    print(f\"找到 {len(files)} 个文档:\")\n",
        "    for file in files:\n",
        "        print(f\"  - {file}\")\n",
        "else:\n",
        "    print(\"没有找到文档目录\")\n",
        "    print(\"当前目录内容:\")\n",
        "    print(os.listdir('/content'))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {"id": "install_packages"},
      "source": [
        "## 2. 安装依赖包"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {"id": "install_packages_code"},
      "outputs": [],
      "source": [
        "# 安装必要的包\n",
        "!pip install torch transformers sentence-transformers faiss-cpu numpy pandas scikit-learn python-docx PyPDF2 beautifulsoup4 tqdm datasets accelerate evaluate\n",
        "\n",
        "# 检查GPU是否可用\n",
        "import torch\n",
        "print(f\"CUDA可用: {torch.cuda.is_available()}\")\n",
        "if torch.cuda.is_available():\n",
        "    print(f\"GPU设备: {torch.cuda.get_device_name(0)}\")\n",
        "    print(f\"GPU内存: {torch.cuda.get_device_properties(0).total_memory / 1024**3:.1f} GB\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {"id": "load_documents"},
      "source": [
        "## 3. 加载文档"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {"id": "load_documents_code"},
      "outputs": [],
      "source": [
        "import os\n",
        "from pathlib import Path\n",
        "\n",
        "# 设置文档路径 - 使用绝对路径\n",
        "DOCS_PATH = '/content/literature-docs/documents'\n",
        "MODELS_PATH = '/content/literature-docs/models'\n",
        "DATA_PATH = '/content/literature-docs/data'\n",
        "\n",
        "print(f\"文档路径: {DOCS_PATH}\")\n",
        "print(f\"模型路径: {MODELS_PATH}\")\n",
        "print(f\"数据路径: {DATA_PATH}\")\n",
        "\n",
        "# 检查路径是否存在\n",
        "print(f\"\\n文档路径存在: {os.path.exists(DOCS_PATH)}\")\n",
        "if os.path.exists(DOCS_PATH):\n",
        "    print(f\"文档目录内容: {os.listdir(DOCS_PATH)}\")\n",
        "else:\n",
        "    print(\"\\n检查 /content 目录:\")\n",
        "    print(os.listdir('/content'))\n",
        "    \n",
        "    print(\"\\n检查 /content/literature-docs 目录:\")\n",
        "    if os.path.exists('/content/literature-docs'):\n",
        "        print(os.listdir('/content/literature-docs'))\n",
        "    else:\n",
        "        print(\"literature-docs 目录不存在\")\n",
        "\n",
        "# 创建必要的目录\n",
        "os.makedirs(MODELS_PATH, exist_ok=True)\n",
        "os.makedirs(DATA_PATH, exist_ok=True)\n",
        "\n",
        "# 获取文档列表\n",
        "if os.path.exists(DOCS_PATH):\n",
        "    doc_files = []\n",
        "    for file in os.listdir(DOCS_PATH):\n",
        "        file_path = os.path.join(DOCS_PATH, file)\n",
        "        if os.path.isfile(file_path):\n",
        "            doc_files.append(file_path)\n",
        "    \n",
        "    print(f\"\\n找到 {len(doc_files)} 个文档:\")\n",
        "    for file_path in doc_files:\n",
        "        print(f\"  - {os.path.basename(file_path)}\")\n",
        "else:\n",
        "    print(\"\\n❌ 没有找到文档文件\")\n",
        "    doc_files = []"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {"id": "create_modules"},
      "source": [
        "## 4. 创建训练模块"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {"id": "create_modules_code"},
      "outputs": [],
      "source": [
        "# 创建完整的训练模块\n",
        "import os\n",
        "import re\n",
        "import json\n",
        "import torch\n",
        "import numpy as np\n",
        "from transformers import (\n",
        "    AutoTokenizer, AutoModelForSequenceClassification, \n",
        "    TrainingArguments, Trainer,\n",
        "    DataCollatorWithPadding\n",
        ")\n",
        "from datasets import Dataset\n",
        "from sklearn.metrics import accuracy_score, precision_recall_fscore_support\n",
        "import PyPDF2\n",
        "from docx import Document\n",
        "from bs4 import BeautifulSoup\n",
        "import logging\n",
        "from datetime import datetime\n",
        "\n",
        "# 设置日志\n",
        "logging.basicConfig(level=logging.INFO)\n",
        "logger = logging.getLogger(__name__)\n",
        "\n",
        "print(\"✅ 训练模块导入完成\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {"id": "create_document_processor"},
      "outputs": [],
      "source": [
        "# 创建文档处理类\n",
        "class DocumentProcessor:\n",
        "    \"\"\"文档处理器\"\"\"\n",
        "    \n",
        "    def __init__(self):\n",
        "        self.supported_formats = ['.pdf', '.docx', '.txt', '.md']\n",
        "    \n",
        "    def extract_text_from_pdf(self, file_path):\n",
        "        \"\"\"从PDF文件中提取文本\"\"\"\n",
        "        try:\n",
        "            with open(file_path, 'rb') as file:\n",
        "                pdf_reader = PyPDF2.PdfReader(file)\n",
        "                text = \"\"\n",
        "                for page in pdf_reader.pages:\n",
        "                    text += page.extract_text() + \"\\n\"\n",
        "                return text.strip()\n",
        "        except Exception as e:\n",
        "            logger.error(f\"PDF处理错误: {e}\")\n",
        "            return \"\"\n",
        "    \n",
        "    def extract_text_from_docx(self, file_path):\n",
        "        \"\"\"从DOCX文件中提取文本\"\"\"\n",
        "        try:\n",
        "            doc = Document(file_path)\n",
        "            text = \"\"\n",
        "            for paragraph in doc.paragraphs:\n",
        "                text += paragraph.text + \"\\n\"\n",
        "            return text.strip()\n",
        "        except Exception as e:\n",
        "            logger.error(f\"DOCX处理错误: {e}\")\n",
        "            return \"\"\n",
        "    \n",
        "    def extract_text_from_txt(self, file_path):\n",
        "        \"\"\"从TXT文件中提取文本\"\"\"\n",
        "        try:\n",
        "            with open(file_path, 'r', encoding='utf-8') as file:\n",
        "                return file.read().strip()\n",
        "        except Exception as e:\n",
        "            logger.error(f\"TXT处理错误: {e}\")\n",
        "            return \"\"\n",
        "    \n",
        "    def clean_text(self, text):\n",
        "        \"\"\"清理文本内容\"\"\"\n",
        "        text = re.sub(r'\\s+', ' ', text)\n",
        "        text = re.sub(r'[^\\u4e00-\\u9fff\\w\\s.,;:!?()（）【】\"\"''""''、。，；：！？]', '', text)\n",
        "        return text.strip()\n",
        "    \n",
        "    def split_into_chunks(self, text, chunk_size=1000, overlap=200):\n",
        "        \"\"\"将长文本分割成较小的块\"\"\"\n",
        "        if len(text) <= chunk_size:\n",
        "            return [text]\n",
        "        \n",
        "        chunks = []\n",
        "        start = 0\n",
        "        \n",
        "        while start < len(text):\n",
        "            end = start + chunk_size\n",
        "            \n",
        "            if end < len(text):\n",
        "                for i in range(end, max(start + chunk_size - 100, start), -1):\n",
        "                    if text[i] in '。！？.!?':\n",
        "                        end = i + 1\n",
        "                        break\n",
        "            \n",
        "            chunk = text[start:end].strip()\n",
        "            if chunk:\n",
        "                chunks.append(chunk)\n",
        "            \n",
        "            start = end - overlap\n",
        "            if start >= len(text):\n",
        "                break\n",
        "        \n",
        "        return chunks\n",
        "    \n",
        "    def process_document(self, file_path):\n",
        "        \"\"\"处理单个文档\"\"\"\n",
        "        file_ext = os.path.splitext(file_path)[1].lower()\n",
        "        \n",
        "        if not os.path.exists(file_path):\n",
        "            raise FileNotFoundError(f\"文件不存在: {file_path}\")\n",
        "        \n",
        "        if not file_ext or file_ext not in self.supported_formats:\n",
        "            raise ValueError(f\"不支持的文件格式: {file_ext}\")\n",
        "        \n",
        "        # 提取文本\n",
        "        if file_ext == '.pdf':\n",
        "            text = self.extract_text_from_pdf(file_path)\n",
        "        elif file_ext == '.docx':\n",
        "            text = self.extract_text_from_docx(file_path)\n",
        "        elif file_ext == '.txt':\n",
        "            text = self.extract_text_from_txt(file_path)\n",
        "        else:\n",
        "            text = \"\"\n",
        "        \n",
        "        # 清理文本\n",
        "        cleaned_text = self.clean_text(text)\n",
        "        \n",
        "        # 分割成块\n",
        "        chunks = self.split_into_chunks(cleaned_text)\n",
        "        \n",
        "        return {\n",
        "            'file_path': file_path,\n",
        "            'file_name': os.path.basename(file_path),\n",
        "            'file_type': file_ext,\n",
        "            'original_text': text,\n",
        "            'cleaned_text': cleaned_text,\n",
        "            'chunks': chunks,\n",
        "            'chunk_count': len(chunks),\n",
        "            'total_length': len(cleaned_text)\n",
        "        }\n",
        "\n",
        "print(\"✅ 文档处理类创建完成\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {"id": "create_model_trainer"},
      "outputs": [],
      "source": [
        "# 创建模型训练类\n",
        "class LiteratureModelTrainer:\n",
        "    \"\"\"文献处理模型训练器\"\"\"\n",
        "    \n",
        "    def __init__(self, base_model_name=\"bert-base-chinese\"):\n",
        "        self.base_model_name = base_model_name\n",
        "        self.tokenizer = None\n",
        "        self.model = None\n",
        "        self.training_data = []\n",
        "        self.model_save_path = \"/content/trained_literature_model\"\n",
        "        \n",
        "        self._load_tokenizer()\n",
        "        self._load_model()\n",
        "    \n",
        "    def _load_tokenizer(self):\n",
        "        \"\"\"加载分词器\"\"\"\n",
        "        try:\n",
        "            self.tokenizer = AutoTokenizer.from_pretrained(self.base_model_name)\n",
        "            logger.info(f\"分词器加载成功: {self.base_model_name}\")\n",
        "        except Exception as e:\n",
        "            logger.error(f\"分词器加载失败: {e}\")\n",
        "            raise\n",
        "    \n",
        "    def _load_model(self):\n",
        "        \"\"\"加载模型\"\"\"\n",
        "        try:\n",
        "            # 使用GPU加速\n",
        "            device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "            \n",
        "            self.model = AutoModelForSequenceClassification.from_pretrained(\n",
        "                self.base_model_name, \n",
        "                num_labels=2  # 二分类：相关/不相关\n",
        "            ).to(device)\n",
        "            \n",
        "            logger.info(f\"模型加载成功: {self.base_model_name} (设备: {device})\")\n",
        "        except Exception as e:\n",
        "            logger.error(f\"模型加载失败: {e}\")\n",
        "            raise\n",
        "    \n",
        "    def prepare_training_data(self, documents, queries=None):\n",
        "        \"\"\"准备训练数据\"\"\"\n",
        "        logger.info(\"开始准备训练数据\")\n",
        "        \n",
        "        if not queries:\n",
        "            queries = self._generate_sample_queries(documents)\n",
        "        \n",
        "        training_examples = []\n",
        "        \n",
        "        for doc in documents:\n",
        "            for chunk in doc['chunks']:\n",
        "                positive_examples = self._create_positive_examples(chunk, doc, queries)\n",
        "                negative_examples = self._create_negative_examples(chunk, doc, documents, queries)\n",
        "                \n",
        "                training_examples.extend(positive_examples)\n",
        "                training_examples.extend(negative_examples)\n",
        "        \n",
        "        self.training_data = training_examples\n",
        "        logger.info(f\"准备了 {len(training_examples)} 个训练样本\")\n",
        "        \n",
        "        return training_examples\n",
        "    \n",
        "    def _generate_sample_queries(self, documents):\n",
        "        \"\"\"生成示例查询\"\"\"\n",
        "        queries = []\n",
        "        \n",
        "        for doc in documents:\n",
        "            text = doc['cleaned_text']\n",
        "            words = text.split()\n",
        "            keywords = [word for word in words if 2 <= len(word) <= 10]\n",
        "            if keywords:\n",
        "                import random\n",
        "                selected = random.sample(keywords, min(3, len(keywords)))\n",
        "                queries.extend(selected)\n",
        "        \n",
        "        return list(set(queries))\n",
        "    \n",
        "    def _create_positive_examples(self, chunk, doc, queries):\n",
        "        \"\"\"创建正样本\"\"\"\n",
        "        examples = []\n",
        "        \n",
        "        words = chunk.split()\n",
        "        keywords = [word for word in words if 2 <= len(word) <= 8]\n",
        "        \n",
        "        for keyword in keywords[:2]:\n",
        "            examples.append({\n",
        "                'query': keyword,\n",
        "                'text': chunk,\n",
        "                'label': 1,\n",
        "                'source': doc['file_name']\n",
        "            })\n",
        "        \n",
        "        return examples\n",
        "    \n",
        "    def _create_negative_examples(self, chunk, doc, documents, queries):\n",
        "        \"\"\"创建负样本\"\"\"\n",
        "        examples = []\n",
        "        \n",
        "        import random\n",
        "        \n",
        "        other_chunks = []\n",
        "        for other_doc in documents:\n",
        "            if other_doc['file_name'] != doc['file_name']:\n",
        "                other_chunks.extend(other_doc['chunks'])\n",
        "        \n",
        "        if other_chunks:\n",
        "            negative_chunks = random.sample(other_chunks, min(2, len(other_chunks)))\n",
        "            \n",
        "            for neg_chunk in negative_chunks:\n",
        "                if queries:\n",
        "                    query = random.choice(queries)\n",
        "                    examples.append({\n",
        "                        'query': query,\n",
        "                        'text': neg_chunk,\n",
        "                        'label': 0,\n",
        "                        'source': 'negative'\n",
        "                    })\n",
        "        \n",
        "        return examples\n",
        "\n",
        "print(\"✅ 模型训练类创建完成\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {"id": "process_documents"},
      "source": [
        "## 5. 处理文档并准备训练数据"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {"id": "process_documents_code"},
      "outputs": [],
      "source": [
        "# 处理文档\n",
        "print(\"📄 开始处理文档...\")\n",
        "\n",
        "# 创建文档处理器\n",
        "processor = DocumentProcessor()\n",
        "\n",
        "# 处理文档\n",
        "if doc_files:\n",
        "    processed_docs = []\n",
        "    for file_path in doc_files:\n",
        "        try:\n",
        "            result = processor.process_document(file_path)\n",
        "            processed_docs.append(result)\n",
        "            print(f\"✅ 成功处理: {result['file_name']} ({result['chunk_count']} 个文本块)\")\n",
        "        except Exception as e:\n",
        "            print(f\"❌ 处理失败 {os.path.basename(file_path)}: {e}\")\n",
        "    \n",
        "    total_chunks = sum(doc['chunk_count'] for doc in processed_docs)\n",
        "    print(f\"\\n🎉 处理完成！\")\n",
        "    print(f\"📊 统计信息:\")\n",
        "    print(f\"  - 成功处理文档: {len(processed_docs)}\")\n",
        "    print(f\"  - 总文本块数: {total_chunks}\")\n",
        "    print(f\"  - 总文本长度: {sum(doc['total_length'] for doc in processed_docs)} 字符\")\n",
        "else:\n",
        "    print(\"❌ 没有找到可处理的文件\")\n",
        "    processed_docs = []"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {"id": "train_model"},
      "source": [
        "## 6. 开始训练模型"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {"id": "train_model_code"},
      "outputs": [],
      "source": [
        "# 开始训练模型\n",
        "if processed_docs:\n",
        "    print(\"🚀 开始训练模型...\")\n",
        "    \n",
        "    # 创建模型训练器\n",
        "    trainer = LiteratureModelTrainer()\n",
        "    \n",
        "    # 准备训练数据\n",
        "    training_data = trainer.prepare_training_data(processed_docs)\n",
        "    \n",
        "    print(f\"📊 训练数据统计:\")\n",
        "    print(f\"  - 训练样本数: {len(training_data)}\")\n",
        "    print(f\"  - 正样本数: {sum(1 for ex in training_data if ex['label'] == 1)}\")\n",
        "    print(f\"  - 负样本数: {sum(1 for ex in training_data if ex['label'] == 0)}\")\n",
        "    \n",
        "    # 对数据进行分词处理\n",
        "    def tokenize_function(examples):\n",
        "        combined_texts = [f\"{q} [SEP] {t}\" for q, t in zip(examples['query'], examples['text'])]\n",
        "        \n",
        "        return trainer.tokenizer(\n",
        "            combined_texts,\n",
        "            truncation=True,\n",
        "            padding=True,\n",
        "            max_length=512,\n",
        "            return_tensors=\"pt\"\n",
        "        )\n",
        "    \n",
        "    # 创建数据集\n",
        "    dataset_dict = {\n",
        "        'query': [ex['query'] for ex in training_data],\n",
        "        'text': [ex['text'] for ex in training_data],\n",
        "        'label': [ex['label'] for ex in training_data],\n",
        "        'source': [ex['source'] for ex in training_data]\n",
        "    }\n",
        "    \n",
        "    dataset = Dataset.from_dict(dataset_dict)\n",
        "    tokenized_dataset = dataset.map(tokenize_function, batched=True)\n",
        "    \n",
        "    # 分割训练和验证集\n",
        "    train_size = int(len(tokenized_dataset) * 0.8)\n",
        "    train_dataset = tokenized_dataset.select(range(train_size))\n",
        "    eval_dataset = tokenized_dataset.select(range(train_size, len(tokenized_dataset)))\n",
        "    \n",
        "    print(f\"  - 训练集大小: {len(train_dataset)}\")\n",
        "    print(f\"  - 验证集大小: {len(eval_dataset)}\")\n",
        "    \n",
        "    # 设置训练参数\n",
        "    training_args = TrainingArguments(\n",
        "        output_dir=\"/content/training_output\",\n",
        "        num_train_epochs=3,\n",
        "        per_device_train_batch_size=16,\n",
        "        per_device_eval_batch_size=16,\n",
        "        warmup_steps=100,\n",
        "        weight_decay=0.01,\n",
        "        learning_rate=2e-5,\n",
        "        logging_dir=\"/content/logs\",\n",
        "        logging_steps=50,\n",
        "        save_steps=500,\n",
        "        eval_steps=500,\n",
        "        eval_strategy=\"steps\",\n",
        "        save_strategy=\"steps\",\n",
        "        load_best_model_at_end=True,\n",
        "        metric_for_best_model=\"f1\",\n",
        "        greater_is_better=True,\n",
        "        report_to=\"none\",\n",
        "    )\n",
        "    \n",
        "    # 创建数据整理器\n",
        "    data_collator = DataCollatorWithPadding(tokenizer=trainer.tokenizer)\n",
        "    \n",
        "    # 计算评估指标\n",
        "    def compute_metrics(eval_pred):\n",
        "        predictions, labels = eval_pred\n",
        "        predictions = np.argmax(predictions, axis=1)\n",
        "        \n",
        "        precision, recall, f1, _ = precision_recall_fscore_support(labels, predictions, average='binary')\n",
        "        accuracy = accuracy_score(labels, predictions)\n",
        "        \n",
        "        return {\n",
        "            'accuracy': accuracy,\n",
        "            'f1': f1,\n",
        "            'precision': precision,\n",
        "            'recall': recall\n",
        "        }\n",
        "    \n",
        "    # 创建训练器\n",
        "    trainer_instance = Trainer(\n",
        "        model=trainer.model,\n",
        "        args=training_args,\n",
        "        train_dataset=train_dataset,\n",
        "        eval_dataset=eval_dataset,\n",
        "        data_collator=data_collator,\n",
        "        compute_metrics=compute_metrics,\n",
        "    )\n",
        "    \n",
        "    print(\"🎯 开始训练...\")\n",
        "    print(\"预计训练时间: 10-30分钟\")\n",
        "    \n",
        "    # 开始训练\n",
        "    train_result = trainer_instance.train()\n",
        "    \n",
        "    # 保存模型\n",
        "    trainer_instance.save_model(trainer.model_save_path)\n",
        "    trainer.tokenizer.save_pretrained(trainer.model_save_path)\n",
        "    \n",
        "    print(\"\\n🎉 训练完成！\")\n",
        "    print(f\"训练损失: {train_result.training_loss:.4f}\")\n",
        "    print(f\"模型保存路径: {trainer.model_save_path}\")\n",
        "    \n",
        "else:\n",
        "    print(\"❌ 没有文档可以训练\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {"id": "download_model"},
      "source": [
        "## 7. 下载训练好的模型"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {"id": "download_model_code"},
      "outputs": [],
      "source": [
        "# 下载训练好的模型\n",
        "import zipfile\n",
        "import shutil\n",
        "\n",
        "# 检查模型是否训练完成\n",
        "if os.path.exists('/content/trained_literature_model'):\n",
        "    print(\"🎉 模型训练完成，准备下载...\")\n",
        "    \n",
        "    # 创建压缩包\n",
        "    zip_filename = 'trained_literature_model.zip'\n",
        "    with zipfile.ZipFile(zip_filename, 'w', zipfile.ZIP_DEFLATED) as zipf:\n",
        "        for root, dirs, files in os.walk('/content/trained_literature_model'):\n",
        "            for file in files:\n",
        "                file_path = os.path.join(root, file)\n",
        "                arcname = os.path.relpath(file_path, '/content')\n",
        "                zipf.write(file_path, arcname)\n",
        "    \n",
        "    print(f\"✅ 模型已打包为: {zip_filename}\")\n",
        "    print(f\"📦 文件大小: {os.path.getsize(zip_filename) / 1024 / 1024:.1f} MB\")\n",
        "    \n",
        "    # 下载文件\n",
        "    from google.colab import files\n",
        "    files.download(zip_filename)\n",
        "    \n",
        "    print(\"\\n🎉 模型下载完成！\")\n",
        "    print(\"\\n📋 使用方法:\")\n",
        "    print(\"1. 解压下载的zip文件\")\n",
        "    print(\"2. 将模型文件夹放到您的项目目录中\")\n",
        "    print(\"3. 在代码中加载自定义模型\")\n",
        "    \n",
        "    # 显示模型信息\n",
        "    print(\"\\n📊 模型信息:\")\n",
        "    print(f\"  - 模型路径: /content/trained_literature_model\")\n",
        "    print(f\"  - 模型类型: BERT-based 二分类模型\")\n",
        "    print(f\"  - 训练数据: {len(processed_docs)} 个文档\")\n",
        "    print(f\"  - 训练样本: {len(training_data)} 个样本\")\n",
        "    \n",
        "else:\n",
        "    print(\"❌ 模型尚未训练完成或训练失败\")"
      ]
    }
  ]
}
