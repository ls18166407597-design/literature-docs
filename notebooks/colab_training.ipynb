{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {"id": "header"},
      "source": [
        "# æ–‡çŒ®æ£€ç´¢ç³»ç»Ÿæ¨¡å‹è®­ç»ƒ\n",
        "\n",
        "æœ¬ç¬”è®°æœ¬è‡ªåŠ¨ä»GitHubåŠ è½½æ–‡æ¡£ï¼Œæ— éœ€æ‰‹åŠ¨ä¸Šä¼ ã€‚\n",
        "\n",
        "## åŠŸèƒ½ç‰¹ç‚¹\n",
        "- è‡ªåŠ¨ä»GitHubå…‹éš†æ–‡æ¡£\n",
        "- ä½¿ç”¨å…è´¹GPUåŠ é€Ÿè®­ç»ƒ\n",
        "- æ”¯æŒPDFã€DOCXã€TXTã€MDæ ¼å¼\n",
        "- è®­ç»ƒå®Œæˆåè‡ªåŠ¨ä¿å­˜"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {"id": "clone_repo"},
      "source": [
        "## 1. å…‹éš†GitHubä»“åº“"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {"id": "clone_repo_code"},
      "outputs": [],
      "source": [
        "# å…‹éš†æ‚¨çš„GitHubä»“åº“\n",
        "!git clone https://github.com/ls18166407597-design/literature-docs.git\n",
        "\n",
        "# æŸ¥çœ‹æ–‡æ¡£\n",
        "import os\n",
        "if os.path.exists('/content/literature-docs/documents'):\n",
        "    files = os.listdir('/content/literature-docs/documents')\n",
        "    print(f\"æ‰¾åˆ° {len(files)} ä¸ªæ–‡æ¡£:\")\n",
        "    for file in files:\n",
        "        print(f\"  - {file}\")\n",
        "else:\n",
        "    print(\"æ²¡æœ‰æ‰¾åˆ°æ–‡æ¡£ç›®å½•\")\n",
        "    print(\"å½“å‰ç›®å½•å†…å®¹:\")\n",
        "    print(os.listdir('/content'))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {"id": "install_packages"},
      "source": [
        "## 2. å®‰è£…ä¾èµ–åŒ…"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {"id": "install_packages_code"},
      "outputs": [],
      "source": [
        "# å®‰è£…å¿…è¦çš„åŒ…\n",
        "!pip install torch transformers sentence-transformers faiss-cpu numpy pandas scikit-learn python-docx PyPDF2 beautifulsoup4 tqdm datasets accelerate evaluate\n",
        "\n",
        "# æ£€æŸ¥GPUæ˜¯å¦å¯ç”¨\n",
        "import torch\n",
        "print(f\"CUDAå¯ç”¨: {torch.cuda.is_available()}\")\n",
        "if torch.cuda.is_available():\n",
        "    print(f\"GPUè®¾å¤‡: {torch.cuda.get_device_name(0)}\")\n",
        "    print(f\"GPUå†…å­˜: {torch.cuda.get_device_properties(0).total_memory / 1024**3:.1f} GB\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {"id": "load_documents"},
      "source": [
        "## 3. åŠ è½½æ–‡æ¡£"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {"id": "load_documents_code"},
      "outputs": [],
      "source": [
        "import os\n",
        "from pathlib import Path\n",
        "\n",
        "# è®¾ç½®æ–‡æ¡£è·¯å¾„ - ä½¿ç”¨ç»å¯¹è·¯å¾„\n",
        "DOCS_PATH = '/content/literature-docs/documents'\n",
        "MODELS_PATH = '/content/literature-docs/models'\n",
        "DATA_PATH = '/content/literature-docs/data'\n",
        "\n",
        "print(f\"æ–‡æ¡£è·¯å¾„: {DOCS_PATH}\")\n",
        "print(f\"æ¨¡å‹è·¯å¾„: {MODELS_PATH}\")\n",
        "print(f\"æ•°æ®è·¯å¾„: {DATA_PATH}\")\n",
        "\n",
        "# æ£€æŸ¥è·¯å¾„æ˜¯å¦å­˜åœ¨\n",
        "print(f\"\\næ–‡æ¡£è·¯å¾„å­˜åœ¨: {os.path.exists(DOCS_PATH)}\")\n",
        "if os.path.exists(DOCS_PATH):\n",
        "    print(f\"æ–‡æ¡£ç›®å½•å†…å®¹: {os.listdir(DOCS_PATH)}\")\n",
        "else:\n",
        "    print(\"\\næ£€æŸ¥ /content ç›®å½•:\")\n",
        "    print(os.listdir('/content'))\n",
        "    \n",
        "    print(\"\\næ£€æŸ¥ /content/literature-docs ç›®å½•:\")\n",
        "    if os.path.exists('/content/literature-docs'):\n",
        "        print(os.listdir('/content/literature-docs'))\n",
        "    else:\n",
        "        print(\"literature-docs ç›®å½•ä¸å­˜åœ¨\")\n",
        "\n",
        "# åˆ›å»ºå¿…è¦çš„ç›®å½•\n",
        "os.makedirs(MODELS_PATH, exist_ok=True)\n",
        "os.makedirs(DATA_PATH, exist_ok=True)\n",
        "\n",
        "# è·å–æ–‡æ¡£åˆ—è¡¨\n",
        "if os.path.exists(DOCS_PATH):\n",
        "    doc_files = []\n",
        "    for file in os.listdir(DOCS_PATH):\n",
        "        file_path = os.path.join(DOCS_PATH, file)\n",
        "        if os.path.isfile(file_path):\n",
        "            doc_files.append(file_path)\n",
        "    \n",
        "    print(f\"\\næ‰¾åˆ° {len(doc_files)} ä¸ªæ–‡æ¡£:\")\n",
        "    for file_path in doc_files:\n",
        "        print(f\"  - {os.path.basename(file_path)}\")\n",
        "else:\n",
        "    print(\"\\nâŒ æ²¡æœ‰æ‰¾åˆ°æ–‡æ¡£æ–‡ä»¶\")\n",
        "    doc_files = []"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {"id": "create_modules"},
      "source": [
        "## 4. åˆ›å»ºè®­ç»ƒæ¨¡å—"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {"id": "create_modules_code"},
      "outputs": [],
      "source": [
        "# åˆ›å»ºå®Œæ•´çš„è®­ç»ƒæ¨¡å—\n",
        "import os\n",
        "import re\n",
        "import json\n",
        "import torch\n",
        "import numpy as np\n",
        "from transformers import (\n",
        "    AutoTokenizer, AutoModelForSequenceClassification, \n",
        "    TrainingArguments, Trainer,\n",
        "    DataCollatorWithPadding\n",
        ")\n",
        "from datasets import Dataset\n",
        "from sklearn.metrics import accuracy_score, precision_recall_fscore_support\n",
        "import PyPDF2\n",
        "from docx import Document\n",
        "from bs4 import BeautifulSoup\n",
        "import logging\n",
        "from datetime import datetime\n",
        "\n",
        "# è®¾ç½®æ—¥å¿—\n",
        "logging.basicConfig(level=logging.INFO)\n",
        "logger = logging.getLogger(__name__)\n",
        "\n",
        "print(\"âœ… è®­ç»ƒæ¨¡å—å¯¼å…¥å®Œæˆ\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {"id": "create_document_processor"},
      "outputs": [],
      "source": [
        "# åˆ›å»ºæ–‡æ¡£å¤„ç†ç±»\n",
        "class DocumentProcessor:\n",
        "    \"\"\"æ–‡æ¡£å¤„ç†å™¨\"\"\"\n",
        "    \n",
        "    def __init__(self):\n",
        "        self.supported_formats = ['.pdf', '.docx', '.txt', '.md']\n",
        "    \n",
        "    def extract_text_from_pdf(self, file_path):\n",
        "        \"\"\"ä»PDFæ–‡ä»¶ä¸­æå–æ–‡æœ¬\"\"\"\n",
        "        try:\n",
        "            with open(file_path, 'rb') as file:\n",
        "                pdf_reader = PyPDF2.PdfReader(file)\n",
        "                text = \"\"\n",
        "                for page in pdf_reader.pages:\n",
        "                    text += page.extract_text() + \"\\n\"\n",
        "                return text.strip()\n",
        "        except Exception as e:\n",
        "            logger.error(f\"PDFå¤„ç†é”™è¯¯: {e}\")\n",
        "            return \"\"\n",
        "    \n",
        "    def extract_text_from_docx(self, file_path):\n",
        "        \"\"\"ä»DOCXæ–‡ä»¶ä¸­æå–æ–‡æœ¬\"\"\"\n",
        "        try:\n",
        "            doc = Document(file_path)\n",
        "            text = \"\"\n",
        "            for paragraph in doc.paragraphs:\n",
        "                text += paragraph.text + \"\\n\"\n",
        "            return text.strip()\n",
        "        except Exception as e:\n",
        "            logger.error(f\"DOCXå¤„ç†é”™è¯¯: {e}\")\n",
        "            return \"\"\n",
        "    \n",
        "    def extract_text_from_txt(self, file_path):\n",
        "        \"\"\"ä»TXTæ–‡ä»¶ä¸­æå–æ–‡æœ¬\"\"\"\n",
        "        try:\n",
        "            with open(file_path, 'r', encoding='utf-8') as file:\n",
        "                return file.read().strip()\n",
        "        except Exception as e:\n",
        "            logger.error(f\"TXTå¤„ç†é”™è¯¯: {e}\")\n",
        "            return \"\"\n",
        "    \n",
        "    def clean_text(self, text):\n",
        "        \"\"\"æ¸…ç†æ–‡æœ¬å†…å®¹\"\"\"\n",
        "        text = re.sub(r'\\s+', ' ', text)\n",
        "        text = re.sub(r'[^\\u4e00-\\u9fff\\w\\s.,;:!?()ï¼ˆï¼‰ã€ã€‘\"\"''""''ã€ã€‚ï¼Œï¼›ï¼šï¼ï¼Ÿ]', '', text)\n",
        "        return text.strip()\n",
        "    \n",
        "    def split_into_chunks(self, text, chunk_size=1000, overlap=200):\n",
        "        \"\"\"å°†é•¿æ–‡æœ¬åˆ†å‰²æˆè¾ƒå°çš„å—\"\"\"\n",
        "        if len(text) <= chunk_size:\n",
        "            return [text]\n",
        "        \n",
        "        chunks = []\n",
        "        start = 0\n",
        "        \n",
        "        while start < len(text):\n",
        "            end = start + chunk_size\n",
        "            \n",
        "            if end < len(text):\n",
        "                for i in range(end, max(start + chunk_size - 100, start), -1):\n",
        "                    if text[i] in 'ã€‚ï¼ï¼Ÿ.!?':\n",
        "                        end = i + 1\n",
        "                        break\n",
        "            \n",
        "            chunk = text[start:end].strip()\n",
        "            if chunk:\n",
        "                chunks.append(chunk)\n",
        "            \n",
        "            start = end - overlap\n",
        "            if start >= len(text):\n",
        "                break\n",
        "        \n",
        "        return chunks\n",
        "    \n",
        "    def process_document(self, file_path):\n",
        "        \"\"\"å¤„ç†å•ä¸ªæ–‡æ¡£\"\"\"\n",
        "        file_ext = os.path.splitext(file_path)[1].lower()\n",
        "        \n",
        "        if not os.path.exists(file_path):\n",
        "            raise FileNotFoundError(f\"æ–‡ä»¶ä¸å­˜åœ¨: {file_path}\")\n",
        "        \n",
        "        if not file_ext or file_ext not in self.supported_formats:\n",
        "            raise ValueError(f\"ä¸æ”¯æŒçš„æ–‡ä»¶æ ¼å¼: {file_ext}\")\n",
        "        \n",
        "        # æå–æ–‡æœ¬\n",
        "        if file_ext == '.pdf':\n",
        "            text = self.extract_text_from_pdf(file_path)\n",
        "        elif file_ext == '.docx':\n",
        "            text = self.extract_text_from_docx(file_path)\n",
        "        elif file_ext == '.txt':\n",
        "            text = self.extract_text_from_txt(file_path)\n",
        "        else:\n",
        "            text = \"\"\n",
        "        \n",
        "        # æ¸…ç†æ–‡æœ¬\n",
        "        cleaned_text = self.clean_text(text)\n",
        "        \n",
        "        # åˆ†å‰²æˆå—\n",
        "        chunks = self.split_into_chunks(cleaned_text)\n",
        "        \n",
        "        return {\n",
        "            'file_path': file_path,\n",
        "            'file_name': os.path.basename(file_path),\n",
        "            'file_type': file_ext,\n",
        "            'original_text': text,\n",
        "            'cleaned_text': cleaned_text,\n",
        "            'chunks': chunks,\n",
        "            'chunk_count': len(chunks),\n",
        "            'total_length': len(cleaned_text)\n",
        "        }\n",
        "\n",
        "print(\"âœ… æ–‡æ¡£å¤„ç†ç±»åˆ›å»ºå®Œæˆ\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {"id": "create_model_trainer"},
      "outputs": [],
      "source": [
        "# åˆ›å»ºæ¨¡å‹è®­ç»ƒç±»\n",
        "class LiteratureModelTrainer:\n",
        "    \"\"\"æ–‡çŒ®å¤„ç†æ¨¡å‹è®­ç»ƒå™¨\"\"\"\n",
        "    \n",
        "    def __init__(self, base_model_name=\"bert-base-chinese\"):\n",
        "        self.base_model_name = base_model_name\n",
        "        self.tokenizer = None\n",
        "        self.model = None\n",
        "        self.training_data = []\n",
        "        self.model_save_path = \"/content/trained_literature_model\"\n",
        "        \n",
        "        self._load_tokenizer()\n",
        "        self._load_model()\n",
        "    \n",
        "    def _load_tokenizer(self):\n",
        "        \"\"\"åŠ è½½åˆ†è¯å™¨\"\"\"\n",
        "        try:\n",
        "            self.tokenizer = AutoTokenizer.from_pretrained(self.base_model_name)\n",
        "            logger.info(f\"åˆ†è¯å™¨åŠ è½½æˆåŠŸ: {self.base_model_name}\")\n",
        "        except Exception as e:\n",
        "            logger.error(f\"åˆ†è¯å™¨åŠ è½½å¤±è´¥: {e}\")\n",
        "            raise\n",
        "    \n",
        "    def _load_model(self):\n",
        "        \"\"\"åŠ è½½æ¨¡å‹\"\"\"\n",
        "        try:\n",
        "            # ä½¿ç”¨GPUåŠ é€Ÿ\n",
        "            device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "            \n",
        "            self.model = AutoModelForSequenceClassification.from_pretrained(\n",
        "                self.base_model_name, \n",
        "                num_labels=2  # äºŒåˆ†ç±»ï¼šç›¸å…³/ä¸ç›¸å…³\n",
        "            ).to(device)\n",
        "            \n",
        "            logger.info(f\"æ¨¡å‹åŠ è½½æˆåŠŸ: {self.base_model_name} (è®¾å¤‡: {device})\")\n",
        "        except Exception as e:\n",
        "            logger.error(f\"æ¨¡å‹åŠ è½½å¤±è´¥: {e}\")\n",
        "            raise\n",
        "    \n",
        "    def prepare_training_data(self, documents, queries=None):\n",
        "        \"\"\"å‡†å¤‡è®­ç»ƒæ•°æ®\"\"\"\n",
        "        logger.info(\"å¼€å§‹å‡†å¤‡è®­ç»ƒæ•°æ®\")\n",
        "        \n",
        "        if not queries:\n",
        "            queries = self._generate_sample_queries(documents)\n",
        "        \n",
        "        training_examples = []\n",
        "        \n",
        "        for doc in documents:\n",
        "            for chunk in doc['chunks']:\n",
        "                positive_examples = self._create_positive_examples(chunk, doc, queries)\n",
        "                negative_examples = self._create_negative_examples(chunk, doc, documents, queries)\n",
        "                \n",
        "                training_examples.extend(positive_examples)\n",
        "                training_examples.extend(negative_examples)\n",
        "        \n",
        "        self.training_data = training_examples\n",
        "        logger.info(f\"å‡†å¤‡äº† {len(training_examples)} ä¸ªè®­ç»ƒæ ·æœ¬\")\n",
        "        \n",
        "        return training_examples\n",
        "    \n",
        "    def _generate_sample_queries(self, documents):\n",
        "        \"\"\"ç”Ÿæˆç¤ºä¾‹æŸ¥è¯¢\"\"\"\n",
        "        queries = []\n",
        "        \n",
        "        for doc in documents:\n",
        "            text = doc['cleaned_text']\n",
        "            words = text.split()\n",
        "            keywords = [word for word in words if 2 <= len(word) <= 10]\n",
        "            if keywords:\n",
        "                import random\n",
        "                selected = random.sample(keywords, min(3, len(keywords)))\n",
        "                queries.extend(selected)\n",
        "        \n",
        "        return list(set(queries))\n",
        "    \n",
        "    def _create_positive_examples(self, chunk, doc, queries):\n",
        "        \"\"\"åˆ›å»ºæ­£æ ·æœ¬\"\"\"\n",
        "        examples = []\n",
        "        \n",
        "        words = chunk.split()\n",
        "        keywords = [word for word in words if 2 <= len(word) <= 8]\n",
        "        \n",
        "        for keyword in keywords[:2]:\n",
        "            examples.append({\n",
        "                'query': keyword,\n",
        "                'text': chunk,\n",
        "                'label': 1,\n",
        "                'source': doc['file_name']\n",
        "            })\n",
        "        \n",
        "        return examples\n",
        "    \n",
        "    def _create_negative_examples(self, chunk, doc, documents, queries):\n",
        "        \"\"\"åˆ›å»ºè´Ÿæ ·æœ¬\"\"\"\n",
        "        examples = []\n",
        "        \n",
        "        import random\n",
        "        \n",
        "        other_chunks = []\n",
        "        for other_doc in documents:\n",
        "            if other_doc['file_name'] != doc['file_name']:\n",
        "                other_chunks.extend(other_doc['chunks'])\n",
        "        \n",
        "        if other_chunks:\n",
        "            negative_chunks = random.sample(other_chunks, min(2, len(other_chunks)))\n",
        "            \n",
        "            for neg_chunk in negative_chunks:\n",
        "                if queries:\n",
        "                    query = random.choice(queries)\n",
        "                    examples.append({\n",
        "                        'query': query,\n",
        "                        'text': neg_chunk,\n",
        "                        'label': 0,\n",
        "                        'source': 'negative'\n",
        "                    })\n",
        "        \n",
        "        return examples\n",
        "\n",
        "print(\"âœ… æ¨¡å‹è®­ç»ƒç±»åˆ›å»ºå®Œæˆ\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {"id": "process_documents"},
      "source": [
        "## 5. å¤„ç†æ–‡æ¡£å¹¶å‡†å¤‡è®­ç»ƒæ•°æ®"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {"id": "process_documents_code"},
      "outputs": [],
      "source": [
        "# å¤„ç†æ–‡æ¡£\n",
        "print(\"ğŸ“„ å¼€å§‹å¤„ç†æ–‡æ¡£...\")\n",
        "\n",
        "# åˆ›å»ºæ–‡æ¡£å¤„ç†å™¨\n",
        "processor = DocumentProcessor()\n",
        "\n",
        "# å¤„ç†æ–‡æ¡£\n",
        "if doc_files:\n",
        "    processed_docs = []\n",
        "    for file_path in doc_files:\n",
        "        try:\n",
        "            result = processor.process_document(file_path)\n",
        "            processed_docs.append(result)\n",
        "            print(f\"âœ… æˆåŠŸå¤„ç†: {result['file_name']} ({result['chunk_count']} ä¸ªæ–‡æœ¬å—)\")\n",
        "        except Exception as e:\n",
        "            print(f\"âŒ å¤„ç†å¤±è´¥ {os.path.basename(file_path)}: {e}\")\n",
        "    \n",
        "    total_chunks = sum(doc['chunk_count'] for doc in processed_docs)\n",
        "    print(f\"\\nğŸ‰ å¤„ç†å®Œæˆï¼\")\n",
        "    print(f\"ğŸ“Š ç»Ÿè®¡ä¿¡æ¯:\")\n",
        "    print(f\"  - æˆåŠŸå¤„ç†æ–‡æ¡£: {len(processed_docs)}\")\n",
        "    print(f\"  - æ€»æ–‡æœ¬å—æ•°: {total_chunks}\")\n",
        "    print(f\"  - æ€»æ–‡æœ¬é•¿åº¦: {sum(doc['total_length'] for doc in processed_docs)} å­—ç¬¦\")\n",
        "else:\n",
        "    print(\"âŒ æ²¡æœ‰æ‰¾åˆ°å¯å¤„ç†çš„æ–‡ä»¶\")\n",
        "    processed_docs = []"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {"id": "train_model"},
      "source": [
        "## 6. å¼€å§‹è®­ç»ƒæ¨¡å‹"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {"id": "train_model_code"},
      "outputs": [],
      "source": [
        "# å¼€å§‹è®­ç»ƒæ¨¡å‹\n",
        "if processed_docs:\n",
        "    print(\"ğŸš€ å¼€å§‹è®­ç»ƒæ¨¡å‹...\")\n",
        "    \n",
        "    # åˆ›å»ºæ¨¡å‹è®­ç»ƒå™¨\n",
        "    trainer = LiteratureModelTrainer()\n",
        "    \n",
        "    # å‡†å¤‡è®­ç»ƒæ•°æ®\n",
        "    training_data = trainer.prepare_training_data(processed_docs)\n",
        "    \n",
        "    print(f\"ğŸ“Š è®­ç»ƒæ•°æ®ç»Ÿè®¡:\")\n",
        "    print(f\"  - è®­ç»ƒæ ·æœ¬æ•°: {len(training_data)}\")\n",
        "    print(f\"  - æ­£æ ·æœ¬æ•°: {sum(1 for ex in training_data if ex['label'] == 1)}\")\n",
        "    print(f\"  - è´Ÿæ ·æœ¬æ•°: {sum(1 for ex in training_data if ex['label'] == 0)}\")\n",
        "    \n",
        "    # å¯¹æ•°æ®è¿›è¡Œåˆ†è¯å¤„ç†\n",
        "    def tokenize_function(examples):\n",
        "        combined_texts = [f\"{q} [SEP] {t}\" for q, t in zip(examples['query'], examples['text'])]\n",
        "        \n",
        "        return trainer.tokenizer(\n",
        "            combined_texts,\n",
        "            truncation=True,\n",
        "            padding=True,\n",
        "            max_length=512,\n",
        "            return_tensors=\"pt\"\n",
        "        )\n",
        "    \n",
        "    # åˆ›å»ºæ•°æ®é›†\n",
        "    dataset_dict = {\n",
        "        'query': [ex['query'] for ex in training_data],\n",
        "        'text': [ex['text'] for ex in training_data],\n",
        "        'label': [ex['label'] for ex in training_data],\n",
        "        'source': [ex['source'] for ex in training_data]\n",
        "    }\n",
        "    \n",
        "    dataset = Dataset.from_dict(dataset_dict)\n",
        "    tokenized_dataset = dataset.map(tokenize_function, batched=True)\n",
        "    \n",
        "    # åˆ†å‰²è®­ç»ƒå’ŒéªŒè¯é›†\n",
        "    train_size = int(len(tokenized_dataset) * 0.8)\n",
        "    train_dataset = tokenized_dataset.select(range(train_size))\n",
        "    eval_dataset = tokenized_dataset.select(range(train_size, len(tokenized_dataset)))\n",
        "    \n",
        "    print(f\"  - è®­ç»ƒé›†å¤§å°: {len(train_dataset)}\")\n",
        "    print(f\"  - éªŒè¯é›†å¤§å°: {len(eval_dataset)}\")\n",
        "    \n",
        "    # è®¾ç½®è®­ç»ƒå‚æ•°\n",
        "    training_args = TrainingArguments(\n",
        "        output_dir=\"/content/training_output\",\n",
        "        num_train_epochs=3,\n",
        "        per_device_train_batch_size=16,\n",
        "        per_device_eval_batch_size=16,\n",
        "        warmup_steps=100,\n",
        "        weight_decay=0.01,\n",
        "        learning_rate=2e-5,\n",
        "        logging_dir=\"/content/logs\",\n",
        "        logging_steps=50,\n",
        "        save_steps=500,\n",
        "        eval_steps=500,\n",
        "        eval_strategy=\"steps\",\n",
        "        save_strategy=\"steps\",\n",
        "        load_best_model_at_end=True,\n",
        "        metric_for_best_model=\"f1\",\n",
        "        greater_is_better=True,\n",
        "        report_to=\"none\",\n",
        "    )\n",
        "    \n",
        "    # åˆ›å»ºæ•°æ®æ•´ç†å™¨\n",
        "    data_collator = DataCollatorWithPadding(tokenizer=trainer.tokenizer)\n",
        "    \n",
        "    # è®¡ç®—è¯„ä¼°æŒ‡æ ‡\n",
        "    def compute_metrics(eval_pred):\n",
        "        predictions, labels = eval_pred\n",
        "        predictions = np.argmax(predictions, axis=1)\n",
        "        \n",
        "        precision, recall, f1, _ = precision_recall_fscore_support(labels, predictions, average='binary')\n",
        "        accuracy = accuracy_score(labels, predictions)\n",
        "        \n",
        "        return {\n",
        "            'accuracy': accuracy,\n",
        "            'f1': f1,\n",
        "            'precision': precision,\n",
        "            'recall': recall\n",
        "        }\n",
        "    \n",
        "    # åˆ›å»ºè®­ç»ƒå™¨\n",
        "    trainer_instance = Trainer(\n",
        "        model=trainer.model,\n",
        "        args=training_args,\n",
        "        train_dataset=train_dataset,\n",
        "        eval_dataset=eval_dataset,\n",
        "        data_collator=data_collator,\n",
        "        compute_metrics=compute_metrics,\n",
        "    )\n",
        "    \n",
        "    print(\"ğŸ¯ å¼€å§‹è®­ç»ƒ...\")\n",
        "    print(\"é¢„è®¡è®­ç»ƒæ—¶é—´: 10-30åˆ†é’Ÿ\")\n",
        "    \n",
        "    # å¼€å§‹è®­ç»ƒ\n",
        "    train_result = trainer_instance.train()\n",
        "    \n",
        "    # ä¿å­˜æ¨¡å‹\n",
        "    trainer_instance.save_model(trainer.model_save_path)\n",
        "    trainer.tokenizer.save_pretrained(trainer.model_save_path)\n",
        "    \n",
        "    print(\"\\nğŸ‰ è®­ç»ƒå®Œæˆï¼\")\n",
        "    print(f\"è®­ç»ƒæŸå¤±: {train_result.training_loss:.4f}\")\n",
        "    print(f\"æ¨¡å‹ä¿å­˜è·¯å¾„: {trainer.model_save_path}\")\n",
        "    \n",
        "else:\n",
        "    print(\"âŒ æ²¡æœ‰æ–‡æ¡£å¯ä»¥è®­ç»ƒ\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {"id": "download_model"},
      "source": [
        "## 7. ä¸‹è½½è®­ç»ƒå¥½çš„æ¨¡å‹"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {"id": "download_model_code"},
      "outputs": [],
      "source": [
        "# ä¸‹è½½è®­ç»ƒå¥½çš„æ¨¡å‹\n",
        "import zipfile\n",
        "import shutil\n",
        "\n",
        "# æ£€æŸ¥æ¨¡å‹æ˜¯å¦è®­ç»ƒå®Œæˆ\n",
        "if os.path.exists('/content/trained_literature_model'):\n",
        "    print(\"ğŸ‰ æ¨¡å‹è®­ç»ƒå®Œæˆï¼Œå‡†å¤‡ä¸‹è½½...\")\n",
        "    \n",
        "    # åˆ›å»ºå‹ç¼©åŒ…\n",
        "    zip_filename = 'trained_literature_model.zip'\n",
        "    with zipfile.ZipFile(zip_filename, 'w', zipfile.ZIP_DEFLATED) as zipf:\n",
        "        for root, dirs, files in os.walk('/content/trained_literature_model'):\n",
        "            for file in files:\n",
        "                file_path = os.path.join(root, file)\n",
        "                arcname = os.path.relpath(file_path, '/content')\n",
        "                zipf.write(file_path, arcname)\n",
        "    \n",
        "    print(f\"âœ… æ¨¡å‹å·²æ‰“åŒ…ä¸º: {zip_filename}\")\n",
        "    print(f\"ğŸ“¦ æ–‡ä»¶å¤§å°: {os.path.getsize(zip_filename) / 1024 / 1024:.1f} MB\")\n",
        "    \n",
        "    # ä¸‹è½½æ–‡ä»¶\n",
        "    from google.colab import files\n",
        "    files.download(zip_filename)\n",
        "    \n",
        "    print(\"\\nğŸ‰ æ¨¡å‹ä¸‹è½½å®Œæˆï¼\")\n",
        "    print(\"\\nğŸ“‹ ä½¿ç”¨æ–¹æ³•:\")\n",
        "    print(\"1. è§£å‹ä¸‹è½½çš„zipæ–‡ä»¶\")\n",
        "    print(\"2. å°†æ¨¡å‹æ–‡ä»¶å¤¹æ”¾åˆ°æ‚¨çš„é¡¹ç›®ç›®å½•ä¸­\")\n",
        "    print(\"3. åœ¨ä»£ç ä¸­åŠ è½½è‡ªå®šä¹‰æ¨¡å‹\")\n",
        "    \n",
        "    # æ˜¾ç¤ºæ¨¡å‹ä¿¡æ¯\n",
        "    print(\"\\nğŸ“Š æ¨¡å‹ä¿¡æ¯:\")\n",
        "    print(f\"  - æ¨¡å‹è·¯å¾„: /content/trained_literature_model\")\n",
        "    print(f\"  - æ¨¡å‹ç±»å‹: BERT-based äºŒåˆ†ç±»æ¨¡å‹\")\n",
        "    print(f\"  - è®­ç»ƒæ•°æ®: {len(processed_docs)} ä¸ªæ–‡æ¡£\")\n",
        "    print(f\"  - è®­ç»ƒæ ·æœ¬: {len(training_data)} ä¸ªæ ·æœ¬\")\n",
        "    \n",
        "else:\n",
        "    print(\"âŒ æ¨¡å‹å°šæœªè®­ç»ƒå®Œæˆæˆ–è®­ç»ƒå¤±è´¥\")"
      ]
    }
  ]
}
